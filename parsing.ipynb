!pip3 install bs4
from bs4 import BeautifulSoup
import requests
import pandas as pd
# Bu, verileri almak istediğimiz web sayfasının url'sidir

url = 'https://bilimgenc.tubitak.gov.tr/bilim-genc-tv'
# Burada bir GET isteği göndermek için requests kitaplığını kullanıyoruz
# belirtilen URL'ye bir yanıt alın
# 200 statüsünü alırsak, her şey yolunda gitti ve sayfa hazır demek

response = requests.get(url)
print(response)
# Yanıt metnini siteden iletiyoruz (response.text)
# Kullanılacak ayrıştırıcıyı belirtin ("lxml"),
# HTML kodunu daha fazla analiz etmek için bir BeautifulSoup nesnesi oluşturduk

bs = BeautifulSoup(response.text,"lxml")
print(bs)
# Burada sayfadaki tüm <div class="article-card best-article-card mb-4 img-bottom-overlay"> öğelerini arıyoruz
# Bunları temp değişkeninde sakladık. Bunlar, toplamak istediğimiz sayfadaki tüm haber sayfalarıdır.

temp = bs.find_all('div', 'article-card best-article-card mb-4 img-bottom-overlay')
print(temp)
# "News" ve "links" anahtarlarına sahip bir sözlük oluşturuyoruz,
# Bu sözlük sırasıyla haber başlıkları ve haber bağlantılarını içerecektir.

dict_news = {"news": [], "links": []}

for i in temp:
    news_text = i.find('p', 'card-text')
    news_link = i.find('a').get('href')
    dict_news["news"].extend(news_text)
    dict_news["links"].append(url + news_link)
# Burada bir veri çerçevesi oluşturmak için pandas kütüphanesini kullanıyoruz
# dict_news sözlüğündeki "haberler" ve "bağlantılar" sütunlarına sahip olacak
# Bu veri çerçevesi toplanan haber verilerini içerecektir

df_news = pd.DataFrame(dict_news, columns=["news", "links"])
# Bu, çıktı kolaylığı içindir.
# Toplanan bilgileri herhangi bir yapıda saklayabilirsiniz

df_news
